# from langchain_community.llms import Ollama
from langchain_ollama.llms import OllamaLLM
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.embeddings import OllamaEmbeddings
from langchain_ollama.llms import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

ollama = Ollama (
    base_url = 'http://localhost:11434',
    model = "llama3.2:latest"
)
# print(ollama.invoke("why is the sky blue"))
#TODO: Load Document
loader = WebBaseLoader("https://www.gutenberg.org/files/1727/1727-h/1727-h.htm")
data = loader.load()
#TODO: Text splitters
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)
all_splits = text_splitter.split_documents(data)
#TODO: Load Embedded model
oembed = OllamaEmbeddings(
    base_url = "http://localhost:11434",
    model = "nomic-embed-text"
)

#TODO: Store model into vector DB
vectorstore = Chroma.from_documents(documents=all_splits, embedding=oembed)
question = "Who is Neleus and who is in Neleus' family?"
docs = vectorstore.similarity_search(question)
len(docs)

#TODO: Ask question and print out result
qachain = RetrievalQA.from_chain_type(ollama, retriever = vectorstore.as_retriever())
res = qachain.invoke({"query": question})
print(res['result'])
